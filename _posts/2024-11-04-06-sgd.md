---
layout: post
title: 06. Stochastic Gradient Descent
mathjax: true
tags:
- Basic Math
- Linear Algebra
categories: BasicMachineLearning
description: Details information about the Stochastic Gradient Descent
---

## Stochastic Gradient Descent

Stochastic Gradient Descent is a variation of the Gradient Descent
algorithm that calculates the gradient based on a single randomly chosen
data point (or sample) rather than using the entire dataset. This
approach speeds up computation, especially for large datasets, but
introduces noise into the parameter updates, causing them to be more
random and potentially unstable. However, this randomness can also help
Stochastic Gradient Descent avoid local optima, making it popular in
training complex machine learning models, especially deep neural
networks.

### How Stochastic Gradient Descent Works

In Stochastic Gradient Descent, instead of calculating the gradient of
the objective function $f(\theta)$ using all training samples, it
calculates the gradient for a single data point $x^{(i)}$ and updates
the parameters accordingly. The update rule is given by:

$$\theta = \theta - \alpha \nabla f(\theta; x^{(i)})$$

where:

-   $\theta$ represents the parameters to be optimized,

-   $\alpha$ is the learning rate,

-   $x^{(i)}$ is a randomly selected data sample, and

-   $\nabla f(\theta; x^{(i)})$ is the gradient of $f$ with respect to
    $\theta$ for sample $x^{(i)}$.

By updating based on a single sample, Stochastic Gradient Descent is
much faster and can start to learn from the data without waiting to
process the entire dataset. However, the inherent noise from using a
single sample's gradient can cause Stochastic Gradient Descent to
oscillate around the minimum rather than converge smoothly.

### Challenges of Stochastic Gradient Descent

Stochastic Gradient Descent faces several challenges, including:

-   **Convergence Stability**: Due to the noisy updates, the path to
    convergence is less stable and may oscillate, especially with high
    learning rates.

-   **Local Optima**: While stochasticity helps in escaping some local
    minima, complex non-convex functions may contain multiple
    problematic local optima or saddle points that can trap the
    optimization process.

-   **Learning Rate Sensitivity**: The choice of learning rate is
    crucial. A high learning rate can cause divergence or oscillation,
    while a low learning rate may lead to slow convergence.

### Methods to Improve Stochastic Gradient Descent Performance and Avoid Local Mininum

To optimize Stochastic Gradient Descent and avoid local mininum or
saddle points, various techniques have been developed:

**Learning Rate Scheduling** Adjusting the learning rate during training
can help improve convergence.

-   **Learning Rate Decay**: Gradually decreasing the learning rate as
    training progresses allows Stochastic Gradient Descent to take large
    steps initially and smaller, more precise steps as it approaches an
    optimum. Common decay strategies include:

    -   *Step Decay*: Reduce the learning rate by a fixed factor at
        regular intervals.

    -   *Exponential Decay*: Multiply the learning rate by a constant
        factor after each epoch.

    -   *Inverse Scaling*: Reduce the learning rate proportional to the
        inverse of the epoch number, defined as:
        $$\alpha_t = \frac{\alpha_0}{1 + \lambda t}$$

-   **Adaptive Learning Rates**: Methods such as AdaGrad, RMSProp, and
    Adam adjust the learning rate adaptively for each parameter. These
    methods use information from previous gradients to scale the
    learning rate, speeding up convergence and helping Stochastic
    Gradient Descent navigate complex landscapes.

**Momentum** Momentum is a technique that helps Stochastic Gradient
Descent build up speed in directions with consistent gradients,
smoothing out noisy updates. The momentum update rule is given by:

$$v_t = \beta v_{t-1} + \alpha \nabla f(\theta; x^{(i)})$$

$$\theta = \theta - v_t$$

where:

-   $v_t$ is the velocity term (accumulated gradient),

-   $\beta$ is the momentum coefficient (typically between 0.5 and 0.9).

By using momentum, Stochastic Gradient Descent retains part of the
previous update direction, effectively accelerating convergence and
reducing oscillations around the minima.

**Batch Normalization** Batch normalization is a technique that
normalizes the input of each layer in neural networks to have zero mean
and unit variance. This normalization can help the optimization
landscape become smoother and more consistent across different regions,
reducing the likelihood of getting trapped in local optima or saddle
points.

**Gradient Noise Injection** Adding random noise to gradients can
prevent the optimizer from getting trapped in sharp local minima. This
noise acts as a perturbation, potentially allowing the algorithm to
escape from narrow local minima. The update rule with noise is:

$$\theta = \theta - \alpha \nabla f(\theta; x^{(i)}) + \epsilon$$

where $\epsilon$ is random noise sampled from a Gaussian or uniform
distribution. The noise level can be reduced over time to help converge
near a minimum.

**Restarts and Ensembles**

-   **Warm Restarts**: A technique where the learning rate is
    periodically reset to a high value to help escape local optima, then
    reduced again. This approach is used in techniques such as Cosine
    Annealing and Stochastic Gradient Descent with Restarts.

-   **Ensemble Methods**: Running multiple Stochastic Gradient Descent
    instances with different initializations or slightly different
    parameters (like learning rate) increases the chances of finding a
    global or near-global minimum. Aggregating solutions from each
    instance can lead to better overall performance.

**Variants and Alternatives to Stochastic Gradient Descent** Several
Stochastic Gradient Descent-based optimizers address specific challenges
and can perform better in complex landscapes:

-   **Adam (Adaptive Moment Estimation)**: Combines ideas from momentum
    and adaptive learning rates, adjusting the step size based on an
    exponentially decaying average of past gradients and squared
    gradients. Adam has shown robust performance and is less sensitive
    to hyperparameter tuning.

-   **Nesterov Accelerated Gradient (NAG)**: An improvement over basic
    momentum, Nesterov momentum calculates the gradient after applying
    the velocity, which gives a "look-ahead" and prevents overshooting.
